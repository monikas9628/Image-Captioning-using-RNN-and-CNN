{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b967d46",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "So basically in an image captioning, we need to predict the caption of the image.To do this we first try to convert the image into a tensor or a matrix and from this we will generate a caption or a sequence of words that has a certain meaning. The sequence is therefore a set of variables which has a meaning and which should come in a certain order. Whenever we are working with sequences, we always think of the RNN (recurrent neural networks). RNN is a mathematical function which helps us in mapping sequences to the vector or vector to sequence or sequence to sequence. Image captioning will come under mapping of vector to sequence. Now, the first task is to get the dense feature vector of the image. For that, we are going to use CNN (Convolutional Neural Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54f4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import encodings.idna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1401f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a31f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "def load_captions (filename):\n",
    "    with open(filename, \"r\") as fp:\n",
    "    # Read all text in the file\n",
    "        text = fp.read()\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341b3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_dict (text):\n",
    "    dict = {}\n",
    "    lines = text.split ('\\n')\n",
    "    for line in lines:\n",
    "    \n",
    "    # Split the dataset into image and caption data\n",
    "        line_split = line.split ('\\t')\n",
    "        if (len(line_split) != 2):\n",
    "            continue\n",
    "        else:\n",
    "            image_data, caption = line_split\n",
    "\n",
    "        image_file, caption_idx = image_data.split ('#')\n",
    "        image_name = image_file.split ('.')[0]\n",
    "    \n",
    "        # if given caption exist already then append it to the existing list otherwise create the new list\n",
    "        if (int(caption_idx) == 0):\n",
    "            dict[image_name] = [caption]\n",
    "        else:\n",
    "            dict[image_name].append (caption)\n",
    "  \n",
    "    return dict\n",
    "\n",
    "doc = load_captions(caption_file)\n",
    "image_dict = captions_dict(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78235a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_image_name(filename):\n",
    "    data = []\n",
    "  \n",
    "    with open(filename, \"r\") as fp:\n",
    "        text = fp.read()\n",
    "  \n",
    "   \n",
    "        lines = text.split ('\\n')\n",
    "        for line in lines:\n",
    "            if (len(line) < 1):\n",
    "                continue\n",
    "            image_name = line.split ('.')[0]\n",
    "      \n",
    "           data.append(image_name)\n",
    "\n",
    "    return set(data)  \n",
    "\n",
    "training_image_name_file = \"Flickr_8k.trainImages.txt\"\n",
    "training_image_names = subset_image_name(training_image_name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a60f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63180e6d",
   "metadata": {},
   "source": [
    "## Pre Processing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5beb3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "    \n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3acfe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [13:16<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "image_dir = \"Flicker8k_Dataset\"\n",
    "training_image_paths = [image_dir +'/'+ name + '.jpg' for name in training_image_names]\n",
    "\n",
    "encode_train = sorted(set(training_image_paths))\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed566bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c6b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each caption for this image\n",
    "# Convert the caption to lowercase, and then remove all special characters from it\n",
    "# Split the caption into separate words, and collect all words which are more than \n",
    "def captions_clean (image_dict):\n",
    "    for key, captions in image_dict.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
    "            clean_words = [word for word in caption_nopunct.split() if ((len(word) > 1) and (word.isalpha()))]\n",
    "            caption_new = ' '.join(clean_words)\n",
    "            captions[i] = caption_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7f5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token (captions):\n",
    "    for i, caption in enumerate (captions):\n",
    "        captions[i] = 'startseq ' + caption + ' endseq'\n",
    "    return (captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddedceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data_dict (image_dict, image_names):\n",
    "    dict = { image_name:add_token(captions) for image_name,captions in image_dict.items() if image_name in image_names}\n",
    "    return (dict)\n",
    "\n",
    "def all_captions (data_dict):\n",
    "    return ([caption for key, captions in data_dict.items() for caption in captions])\n",
    "\n",
    "def max_caption_length(captions):\n",
    "    return max(len(caption.split()) for caption in captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73a3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(data_dict):\n",
    "    captions = all_captions(data_dict)\n",
    "    max_caption_words = max_caption_length(captions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    return (tokenizer, vocab_size, max_caption_words)\n",
    "\n",
    "def pad_text (text, max_length): \n",
    "    text = pad_sequences([text], maxlen=max_length, padding='post')[0]\n",
    "  \n",
    "    return (text)\n",
    "\n",
    "captions_clean (image_dict)\n",
    "training_dict = subset_data_dict(image_dict, training_image_names)\n",
    "\n",
    "tokenizer, vocab_size, max_caption_words = create_tokenizer(training_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7d9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_dict, tokenizer, max_length, vocab_size):\n",
    "    X, y = list(), list()\n",
    "    for image_name, captions in data_dict.items():\n",
    "        image_name = image_dir+'/'+image_name + '.jpg'\n",
    "        for caption in captions:\n",
    "            word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "            pad_idxs = pad_text(word_idxs, max_length)\n",
    "          \n",
    "            X.append(image_name)\n",
    "            y.append(pad_idxs)\n",
    "     \n",
    "    return np.array(X) , np.array(y)\n",
    "    \n",
    "\n",
    "train_X, train_y = data_prep(training_dict, tokenizer, max_caption_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd96a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379cd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa6038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d89cb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "005d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = vocab_size\n",
    "num_steps = len(train_X) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    " \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c043043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.5056\n",
      "Epoch 1 Batch 100 Loss 1.6166\n",
      "Epoch 1 Batch 200 Loss 1.4784\n",
      "Epoch 1 Batch 300 Loss 1.2456\n",
      "Epoch 1 Batch 400 Loss 1.1728\n",
      "Epoch 1 Loss 1.391207\n",
      "Time taken for 1 epoch 1425.59 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1015\n",
      "Epoch 2 Batch 100 Loss 1.1735\n",
      "Epoch 2 Batch 200 Loss 1.1120\n",
      "Epoch 2 Batch 300 Loss 1.0639\n",
      "Epoch 2 Batch 400 Loss 0.8868\n",
      "Epoch 2 Loss 1.086068\n",
      "Time taken for 1 epoch 1400.97 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1003\n",
      "Epoch 3 Batch 100 Loss 0.9958\n",
      "Epoch 3 Batch 200 Loss 0.9792\n",
      "Epoch 3 Batch 300 Loss 0.8266\n",
      "Epoch 3 Batch 400 Loss 0.9130\n",
      "Epoch 3 Loss 0.976447\n",
      "Time taken for 1 epoch 1465.09 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9973\n",
      "Epoch 4 Batch 100 Loss 0.9466\n",
      "Epoch 4 Batch 200 Loss 0.8365\n",
      "Epoch 4 Batch 300 Loss 0.8591\n",
      "Epoch 4 Batch 400 Loss 0.8097\n",
      "Epoch 4 Loss 0.900383\n",
      "Time taken for 1 epoch 1372.54 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.8908\n",
      "Epoch 5 Batch 100 Loss 0.8752\n",
      "Epoch 5 Batch 200 Loss 0.8592\n",
      "Epoch 5 Batch 300 Loss 0.8241\n",
      "Epoch 5 Batch 400 Loss 0.9286\n",
      "Epoch 5 Loss 0.838883\n",
      "Time taken for 1 epoch 1379.20 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.8642\n",
      "Epoch 6 Batch 100 Loss 0.7605\n",
      "Epoch 6 Batch 200 Loss 0.8182\n",
      "Epoch 6 Batch 300 Loss 0.7610\n",
      "Epoch 6 Batch 400 Loss 0.7075\n",
      "Epoch 6 Loss 0.783871\n",
      "Time taken for 1 epoch 1350.28 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.7213\n",
      "Epoch 7 Batch 100 Loss 0.7382\n",
      "Epoch 7 Batch 200 Loss 0.7899\n",
      "Epoch 7 Batch 300 Loss 0.7013\n",
      "Epoch 7 Batch 400 Loss 0.7847\n",
      "Epoch 7 Loss 0.737875\n",
      "Time taken for 1 epoch 1583.96 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_epoch = 0\n",
    "EPOCHS = 7\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68659901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b2f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Caption: ['man in black fedora smiles at the camera', 'man in hat posing for picture', 'an older man in weathered fedora smiles slightly for the camera', 'an old man with blue eyes wears big black hat', 'the man is wearing black hat beige shirt and has his sunglasses hanging off the shirt']\n",
      "Prediction Caption: person wearing hat is wearing black on his head endseq\n"
     ]
    }
   ],
   "source": [
    "def evaluate(image, max_length):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == 'endseq':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "  \n",
    "def check_test(test_image_names, image_dict, image_dir, max_caption_words):\n",
    "  # captions on the validation set\n",
    "    rid = np.random.randint(0, len(test_image_names))\n",
    "    image_name = test_image_names[rid]\n",
    "    real_caption = image_dict[image_name]\n",
    "\n",
    "    image_path = image_dir +'/'+ image_name + '.jpg'\n",
    "    result, attention_plot = evaluate(image_path, max_caption_words)\n",
    "\n",
    "  #from IPython.display import Image, display\n",
    "  #display(Image(image_path))\n",
    "    print('Real Caption:', real_caption)\n",
    "    print('Prediction Caption:', ' '.join(result))\n",
    "test_image_name_file = \"Flickr8k_text/Flickr_8k.testImages.txt\"\n",
    "test_image_names = subset_image_name(test_image_name_file)\n",
    "image_dir = \"Flicker8k_Dataset\"\n",
    "check_test(list(test_image_names), image_dict, image_dir, max_caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17586039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
